{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by specifying: 1. The S3 bucket and prefix to use for the training and model data. This should be within the same region as the Notebook Instance, training, and hosting. 1. The IAM role ARN used to grant access to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-dist-script\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the dataset\n",
    "\n",
    "The following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def data_split(\n",
    "    FILE_DATA,\n",
    "    DATA_DIR,\n",
    "    FILE_TRAIN_BASE,\n",
    "    FILE_TRAIN_1,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN_0,\n",
    "    PERCENT_TRAIN_1,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    "):\n",
    "    data = [l for l in open(FILE_DATA, \"r\")]\n",
    "    train_file_0 = open(DATA_DIR + \"/\" + FILE_TRAIN_0, \"w\")\n",
    "    train_file_1 = open(DATA_DIR + \"/\" + FILE_TRAIN_1, \"w\")\n",
    "    valid_file = open(DATA_DIR + \"/\" + FILE_VALIDATION, \"w\")\n",
    "    tests_file = open(DATA_DIR + \"/\" + FILE_TEST, \"w\")\n",
    "\n",
    "    num_of_data = len(data)\n",
    "    num_train_0 = int((PERCENT_TRAIN_0 / 100.0) * num_of_data)\n",
    "    num_train_1 = int((PERCENT_TRAIN_1 / 100.0) * num_of_data)\n",
    "    num_valid = int((PERCENT_VALIDATION / 100.0) * num_of_data)\n",
    "    num_tests = int((PERCENT_TEST / 100.0) * num_of_data)\n",
    "\n",
    "    data_fractions = [num_train_0, num_train_1, num_valid, num_tests]\n",
    "    split_data = [[], [], [], []]\n",
    "\n",
    "    rand_data_ind = 0\n",
    "\n",
    "    for split_ind, fraction in enumerate(data_fractions):\n",
    "        for i in range(fraction):\n",
    "            rand_data_ind = random.randint(0, len(data) - 1)\n",
    "            split_data[split_ind].append(data[rand_data_ind])\n",
    "            data.pop(rand_data_ind)\n",
    "\n",
    "    for l in split_data[0]:\n",
    "        train_file_0.write(l)\n",
    "\n",
    "    for l in split_data[1]:\n",
    "        train_file_1.write(l)\n",
    "\n",
    "    for l in split_data[2]:\n",
    "        valid_file.write(l)\n",
    "\n",
    "    for l in split_data[3]:\n",
    "        tests_file.write(l)\n",
    "\n",
    "    train_file_0.close()\n",
    "    train_file_1.close()\n",
    "    valid_file.close()\n",
    "    tests_file.close()\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj = open(filename, \"rb\")\n",
    "    key = prefix + \"/\" + channel\n",
    "    url = \"s3://{}/{}/{}\".format(bucket, key, filename)\n",
    "    print(\"Writing to {}\".format(url))\n",
    "    write_to_s3(fobj, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done in situ by SageMaker Processing, Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn’t onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Load the dataset\n",
    "FILE_DATA = \"abalone\"\n",
    "s3.download_file(\n",
    "    \"sagemaker-sample-files\", f\"datasets/tabular/uci_abalone/abalone.libsvm\", FILE_DATA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN_0 = \"abalone.train_0\"\n",
    "FILE_TRAIN_1 = \"abalone.train_1\"\n",
    "FILE_VALIDATION = \"abalone.validation\"\n",
    "FILE_TEST = \"abalone.test\"\n",
    "PERCENT_TRAIN_0 = 35\n",
    "PERCENT_TRAIN_1 = 35\n",
    "PERCENT_VALIDATION = 15\n",
    "PERCENT_TEST = 15\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "data_split(\n",
    "    FILE_DATA,\n",
    "    DATA_DIR,\n",
    "    FILE_TRAIN_0,\n",
    "    FILE_TRAIN_1,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN_0,\n",
    "    PERCENT_TRAIN_1,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3://sagemaker-us-east-1-103721820087/sagemaker/DEMO-xgboost-dist-script/train/train_0.libsvm/data/abalone.train_0\n",
      "Writing to s3://sagemaker-us-east-1-103721820087/sagemaker/DEMO-xgboost-dist-script/train/train_1.libsvm/data/abalone.train_1\n",
      "Writing to s3://sagemaker-us-east-1-103721820087/sagemaker/DEMO-xgboost-dist-script/validation/validation.libsvm/data/abalone.validation\n",
      "Writing to s3://sagemaker-us-east-1-103721820087/sagemaker/DEMO-xgboost-dist-script/test/test.libsvm/data/abalone.test\n"
     ]
    }
   ],
   "source": [
    "# Upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train/train_0.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_0)\n",
    "upload_to_s3(bucket, \"train/train_1.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_1)\n",
    "upload_to_s3(bucket, \"validation/validation.libsvm\", DATA_DIR + \"/\" + FILE_VALIDATION)\n",
    "upload_to_s3(bucket, \"test/test.libsvm\", DATA_DIR + \"/\" + FILE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an XGBoost training script\n",
    "\n",
    "SageMaker can now run an XGBoost script using the XGBoost estimator. When run on SageMaker, a number of helpful environment variables are available to access properties of the training environment, such as:\n",
    "\n",
    "- SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "\n",
    "- SM_OUTPUT_DIR: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "When two input channels, train and validation, are used in the call to the XGBoost estimator’s fit() method, the following environment variables are set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "- SM_CHANNEL_TRAIN: A string representing the path to the directory containing data in the ‘train’ channel.\n",
    "\n",
    "- SM_CHANNEL_VALIDATION: Same as above, but for the ‘validation’ channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to the model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance. For example, the script that we run in this notebook is provided as the accompanying file (abalone.py) and also shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m entry_point\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_xgboost_container\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_dmatrix\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_xgboost_container\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m distributed\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mxgboost\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mxgb\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_xgb_train\u001b[39;49;00m(params, dtrain, evals, num_boost_round, model_dir, is_master):\n",
      "    \u001b[33m\"\"\"Run xgb train on arguments given with rabit initialized.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    This is our rabit execution function.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    :param args_dict: Argument dictionary used to run xgb.train().\u001b[39;49;00m\n",
      "\u001b[33m    :param is_master: True if current node is master host in distributed training,\u001b[39;49;00m\n",
      "\u001b[33m                        or is running single node training job.\u001b[39;49;00m\n",
      "\u001b[33m                        Note that rabit_run includes this argument.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    booster = xgb.train(params=params,\n",
      "                        dtrain=dtrain,\n",
      "                        evals=evals,\n",
      "                        num_boost_round=num_boost_round)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\n",
      "        model_location = model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/xgboost-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        pkl.dump(booster, \u001b[36mopen\u001b[39;49;00m(model_location, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "        logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStored trained model at \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_location))\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Hyperparameters are described here.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--min_child_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--subsample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--verbosity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--objective\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_round\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--tree_method\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--predictor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Sagemaker specific arguments. Defaults are set in the environment variables.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm_hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm_current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Get SageMaker host information from runtime environment variables\u001b[39;49;00m\n",
      "    sm_hosts = json.loads(args.sm_hosts)\n",
      "    sm_current_host = args.sm_current_host\n",
      "\n",
      "    dtrain = get_dmatrix(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mlibsvm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    dval = get_dmatrix(args.validation, \u001b[33m'\u001b[39;49;00m\u001b[33mlibsvm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    watchlist = [(dtrain, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), (dval, \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)] \u001b[34mif\u001b[39;49;00m dval \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m [(dtrain, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\n",
      "\n",
      "    train_hp = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmax_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.max_depth,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33meta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.eta,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.gamma,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmin_child_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.min_child_weight,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msubsample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.subsample,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mverbosity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.verbosity,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mobjective\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.objective,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mtree_method\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.tree_method,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mpredictor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.predictor,\n",
      "    }\n",
      "\n",
      "    xgb_train_args = \u001b[36mdict\u001b[39;49;00m(\n",
      "        params=train_hp,\n",
      "        dtrain=dtrain,\n",
      "        evals=watchlist,\n",
      "        num_boost_round=args.num_round,\n",
      "        model_dir=args.model_dir)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(sm_hosts) > \u001b[34m1\u001b[39;49;00m:\n",
      "        \u001b[37m# Wait until all hosts are able to find each other\u001b[39;49;00m\n",
      "        entry_point._wait_hostname_resolution()\n",
      "\n",
      "        \u001b[37m# Execute training function after initializing rabit.\u001b[39;49;00m\n",
      "        distributed.rabit_run(\n",
      "            exec_fun=_xgb_train,\n",
      "            args=xgb_train_args,\n",
      "            include_in_training=(dtrain \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m),\n",
      "            hosts=sm_hosts,\n",
      "            current_host=sm_current_host,\n",
      "            update_rabit_args=\u001b[34mTrue\u001b[39;49;00m\n",
      "        )\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[37m# If single node training, call training method directly.\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m dtrain:\n",
      "            xgb_train_args[\u001b[33m'\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[34mTrue\u001b[39;49;00m\n",
      "            _xgb_train(**xgb_train_args)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining channel must have data to train model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Deserialize and return fitted model.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Note that this should have the same name as the serialized model in the _xgb_train method\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    model_file = \u001b[33m'\u001b[39;49;00m\u001b[33mxgboost-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    booster = pkl.load(\u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, model_file), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m booster\n"
     ]
    }
   ],
   "source": [
    "!pygmentize abalone.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is complete.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.xgboost.estimator.XGBoost estimator, which accepts several constructor arguments:\n",
    "\n",
    "- entry_point: The path to the Python script that SageMaker runs for training and prediction.\n",
    "\n",
    "- role: Role ARN\n",
    "\n",
    "- train_instance_type (optional): The type of SageMaker instances for training.\n",
    "\n",
    "- sagemaker_session (optional): The session used to train on SageMaker.\n",
    "\n",
    "- hyperparameters (optional): A dictionary passed to the train function as hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"abalone-dist-xgb\")\n",
    "content_type = \"libsvm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Source distributed script mode\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "session = Session()\n",
    "script_path = \"abalone.py\"\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(\n",
    "    entry_point=script_path,\n",
    "    framework_version=\"1.5-1\",  # Note: framework_version is mandatory\n",
    "    hyperparameters=hyperparams,\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}/\".format(bucket, prefix, \"train\"), content_type=content_type\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}/\".format(bucket, prefix, \"validation\"), content_type=content_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an XGBoost Estimator on the Abalone data\n",
    "\n",
    "Training is as simple as calling fit() on the Estimator. This starts a SageMaker training job that downloads the data, invokes the entry point code (in the provided script file), and saves any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:49:09 Starting - Starting the training job...ProfilerReport-1669459749: InProgress\n",
      "...\n",
      "2022-11-26 10:49:52 Starting - Preparing the instances for training...\n",
      "2022-11-26 10:50:32 Downloading - Downloading input data...\n",
      "2022-11-26 10:50:52 Training - Downloading the training image...\n",
      "2022-11-26 10:51:33 Training - Training image download completed. Training in progress.\u001b[35m[2022-11-26 10:51:24.448 ip-10-2-210-198.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Module abalone does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:24:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: abalone\n",
      "  Building wheel for abalone (setup.py): started\n",
      "  Building wheel for abalone (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone: filename=abalone-1.0.0-py2.py3-none-any.whl size=5674 sha256=eeae1c951f2ced7ad063e25fc13fc103a7f62f7f1df0ee2946ee510d3cfdcb63\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-pazhva9_/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[35mSuccessfully built abalone\u001b[0m\n",
      "\u001b[35mInstalling collected packages: abalone\u001b[0m\n",
      "\u001b[35mSuccessfully installed abalone-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2022-11-26:10:51:26:INFO] Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-xgboost-2022-11-26-10-49-09-098\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=abalone.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=abalone\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-xgboost-2022-11-26-10-49-09-098\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\",\"module_name\":\"abalone\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[35mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[35mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[35mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[35mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:51:24.348 ip-10-2-201-170.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Module abalone does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:24:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: abalone\n",
      "  Building wheel for abalone (setup.py): started\n",
      "  Building wheel for abalone (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone: filename=abalone-1.0.0-py2.py3-none-any.whl size=5674 sha256=0867640a8ac18db1df1fcc0eaa866f657a8ec50bf0e70b6e7d043838b5098517\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-m5g1ysyo/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built abalone\u001b[0m\n",
      "\u001b[34mInstalling collected packages: abalone\u001b[0m\n",
      "\u001b[34mSuccessfully installed abalone-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-26:10:51:26:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2022-11-26-10-49-09-098\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=abalone.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=abalone\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2022-11-26-10-49-09-098\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-103721820087/sagemaker-xgboost-2022-11-26-10-49-09-098/source/sourcedir.tar.gz\",\"module_name\":\"abalone\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001b[0m\n",
      "\u001b[34m[10:51:27] task NULL got new rank 0\u001b[0m\n",
      "\u001b[35m[10:51:27] task NULL got new rank 1\u001b[0m\n",
      "\u001b[34m[10:51:31] task NULL got new rank 0\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] task NULL got new rank 1\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:31] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:32] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:33] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[10:51:34] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\n",
      "2022-11-26 10:52:13 Uploading - Uploading generated training model\n",
      "2022-11-26 10:52:13 Completed - Training job completed\n",
      "Training seconds: 184\n",
      "Billable seconds: 184\n"
     ]
    }
   ],
   "source": [
    "xgb_script_mode_estimator.fit({\"train\": train_input, \"validation\": validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the XGBoost model\n",
    "\n",
    "After training, we use the estimator to create an Amazon SageMaker endpoint – a hosted and managed prediction service to perform inference.\n",
    "\n",
    "You can also optionally specify other functions to customize the deserialization of the input request (input_fn()), serialization of the predictions (output_fn()), and how predictions are made (predict_fn()). The defaults work for our current use-case so we don’t need to define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = xgb_script_mode_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = DATA_DIR + \"/\" + FILE_TEST\n",
    "with open(test_file, \"r\") as f:\n",
    "    payload = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abalone.test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values are [10.623578071594238, 7.341342926025391, 13.998539924621582, 7.619755744934082, 11.44434928894043, 12.548134803771973, 11.751919746398926, 19.64179039001465, 9.819439888000488, 8.777353286743164, 9.324228286743164, 8.244194984436035, 12.216837882995605, 8.815969467163086, 13.555924415588379, 10.151961326599121, 11.198446273803711, 10.439631462097168, 9.022027015686035, 10.92051887512207, 6.6454877853393555, 11.439027786254883, 8.975362777709961, 8.689529418945312, 13.65711498260498, 6.642266750335693, 10.187984466552734, 12.229193687438965, 10.684317588806152, 11.267091751098633, 11.386499404907227, 9.72307014465332, 7.123825550079346, 12.945900917053223, 10.218866348266602, 20.112186431884766, 11.871651649475098, 7.491597652435303, 10.606607437133789, 11.973468780517578, 7.996725082397461, 16.567672729492188, 10.705711364746094, 9.892657279968262, 6.224245548248291, 6.50828218460083, 10.298068046569824, 9.435389518737793, 10.863693237304688, 12.21094036102295, 13.18262767791748, 5.030646324157715, 10.191386222839355, 6.6454877853393555, 11.458123207092285, 8.602790832519531, 6.0755815505981445, 9.804862022399902, 11.118119239807129, 6.642266750335693, 9.402297973632812, 6.74605131149292, 11.403665542602539, 9.585278511047363, 10.115875244140625, 14.872535705566406, 14.144750595092773, 8.277067184448242, 6.5345540046691895, 9.162154197692871, 13.876956939697266, 10.076783180236816, 9.34737777709961, 5.555413722991943, 11.142547607421875, 11.009393692016602, 10.219454765319824, 6.835484981536865, 8.824385643005371, 11.4005765914917, 11.219042778015137, 7.90491247177124, 10.558429718017578, 9.624744415283203, 7.069905757904053, 7.575442314147949, 8.193872451782227, 9.740294456481934, 10.797889709472656, 7.105839729309082, 16.051664352416992, 9.949922561645508, 10.996905326843262, 8.298074722290039, 10.677607536315918, 10.816006660461426, 9.846291542053223, 9.79295539855957, 10.52441692352295, 19.592973709106445, 14.136109352111816, 10.300004005432129, 10.049359321594238, 7.170929908752441, 8.824385643005371, 16.148853302001953, 7.347826957702637, 8.48611831665039, 8.992822647094727, 10.490257263183594, 9.990893363952637, 10.980806350708008, 9.940110206604004, 6.467811584472656, 4.206228256225586, 10.031249046325684, 6.977365970611572, 10.2861967086792, 9.820716857910156, 11.78329086303711, 13.860575675964355, 12.004972457885742, 5.159342288970947, 10.071059226989746, 3.947657585144043, 7.271493434906006, 14.650550842285156, 15.252264976501465, 9.938333511352539, 6.2884602546691895, 8.180267333984375, 11.61901569366455, 12.344206809997559, 5.2397050857543945, 7.688672065734863, 6.885458946228027, 9.840110778808594, 9.29318618774414, 11.839947700500488, 11.296412467956543, 7.3946638107299805, 10.66015338897705, 13.648811340332031, 4.481322765350342, 20.96708106994629, 7.607729434967041, 10.150160789489746, 10.855097770690918, 9.298245429992676, 9.112142562866211, 10.388912200927734, 7.177590370178223, 15.231221199035645, 6.101095199584961, 7.622572898864746, 8.349091529846191, 8.339186668395996, 12.052706718444824, 11.0868558883667, 6.82313871383667, 11.648265838623047, 10.203472137451172, 8.786693572998047, 8.149677276611328, 6.835484981536865, 7.474822044372559, 11.187097549438477, 8.800357818603516, 7.857831954956055, 12.007428169250488, 8.99226188659668, 8.297972679138184, 9.903791427612305, 10.683821678161621, 11.925446510314941, 9.378092765808105, 9.45089054107666, 12.230731964111328, 8.308170318603516, 10.712246894836426, 18.61041259765625, 11.80077075958252, 9.538737297058105, 5.1930623054504395, 9.880257606506348, 11.670503616333008, 8.60362434387207, 10.029170989990234, 7.243295192718506, 12.20573616027832, 10.379974365234375, 14.683121681213379, 7.862930774688721, 6.616116523742676, 11.540843963623047, 9.312019348144531, 7.354859352111816, 9.431952476501465, 7.071249485015869, 6.509398460388184, 10.943069458007812, 11.509056091308594, 8.57364273071289, 9.719971656799316, 16.066755294799805, 9.681109428405762, 18.039365768432617, 6.398837089538574, 8.552184104919434, 10.385808944702148, 9.412870407104492, 13.887360572814941, 9.363898277282715, 12.758853912353516, 12.721553802490234, 10.207374572753906, 13.322604179382324, 10.1068115234375, 10.553328514099121, 12.129796981811523, 7.943744659423828, 8.23172664642334, 6.4085798263549805, 11.172560691833496, 8.82664966583252, 9.290205955505371, 9.092619895935059, 10.669794082641602, 10.626054763793945, 10.135641098022461, 9.65118408203125, 11.383956909179688, 10.764564514160156, 9.683682441711426, 16.09474754333496, 17.106868743896484, 5.159342288970947, 9.301223754882812, 8.056214332580566, 14.913126945495605, 9.446249961853027, 7.995416641235352, 10.524674415588379, 8.108057022094727, 7.587570667266846, 7.191299915313721, 10.276670455932617, 12.386591911315918, 14.823225021362305, 7.007177352905273, 7.2336554527282715, 10.153413772583008, 10.979308128356934, 11.61612606048584, 13.989441871643066, 9.997340202331543, 7.040091514587402, 7.208319664001465, 11.688252449035645, 8.966694831848145, 9.740352630615234, 11.569975852966309, 8.278970718383789, 9.722311973571777, 15.98835277557373, 8.059128761291504, 14.037528038024902, 6.4627814292907715, 10.53492546081543, 9.043732643127441, 10.275322914123535, 12.558953285217285, 10.668612480163574, 11.221409797668457, 8.827513694763184, 8.402142524719238, 11.008532524108887, 10.595810890197754, 11.321355819702148, 12.110138893127441, 9.391233444213867, 9.57888412475586, 10.097868919372559, 10.417688369750977, 10.871371269226074, 10.316039085388184, 9.168153762817383, 8.820195198059082, 10.266403198242188, 13.536861419677734, 8.215025901794434, 11.198511123657227, 8.870787620544434, 8.836201667785645, 13.076749801635742, 9.52907943725586, 13.010741233825684, 12.429512023925781, 10.232146263122559, 11.574545860290527, 12.28370475769043, 10.97543716430664, 10.09221363067627, 10.751392364501953, 12.782293319702148, 8.045473098754883, 7.1477789878845215, 11.461031913757324, 7.170929908752441, 9.122782707214355, 10.056498527526855, 3.947657585144043, 7.625844955444336, 8.205388069152832, 5.159342288970947, 11.74521541595459, 11.87708568572998, 12.322033882141113, 13.599556922912598, 8.138092994689941, 8.175683975219727, 8.238842964172363, 11.325163841247559, 10.875337600708008, 5.737247467041016, 9.533956527709961, 5.159342288970947, 12.38582706451416, 13.449609756469727, 11.99462604522705, 13.619601249694824, 10.53221607208252, 10.637293815612793, 13.554461479187012, 8.82951831817627, 9.035127639770508, 10.037168502807617, 7.024538516998291, 7.297692775726318, 8.475662231445312, 8.516549110412598, 9.498004913330078, 9.07729721069336, 12.178840637207031, 9.944772720336914, 7.70432710647583, 8.769308090209961, 7.792107105255127, 5.8402581214904785, 7.724560260772705, 6.628023147583008, 7.78618860244751, 11.338915824890137, 8.101991653442383, 8.366031646728516, 10.556347846984863, 10.235730171203613, 4.855917453765869, 15.2778902053833, 8.74000072479248, 7.545630931854248, 9.027125358581543, 7.9604387283325195, 13.630962371826172, 8.611212730407715, 9.171721458435059, 10.421131134033203, 10.444177627563477, 12.608070373535156, 14.006552696228027, 5.555413722991943, 8.568426132202148, 9.182528495788574, 12.056522369384766, 6.581654071807861, 13.970928192138672, 10.580649375915527, 10.439523696899414, 10.681299209594727, 6.220296859741211, 10.093771934509277, 9.744550704956055, 10.135396003723145, 10.952352523803711, 10.223442077636719, 6.657834529876709, 9.470125198364258, 10.552666664123535, 10.183557510375977, 12.394831657409668, 10.337945938110352, 10.164656639099121, 11.977444648742676, 10.559592247009277, 10.17044448852539, 12.262313842773438, 10.552720069885254, 9.128361701965332, 7.98633337020874, 11.019227027893066, 9.230687141418457, 8.067480087280273, 9.39633560180664, 9.735115051269531, 8.855781555175781, 8.478793144226074, 8.258142471313477, 10.66552448272705, 12.050701141357422, 10.07509994506836, 7.792785167694092, 13.40160083770752, 9.290307998657227, 10.591476440429688, 10.928682327270508, 14.283037185668945, 9.212299346923828, 10.356231689453125, 10.710744857788086, 8.229071617126465, 9.608766555786133, 8.66347885131836, 7.043703556060791, 12.010574340820312, 11.218192100524902, 6.784597873687744, 10.960953712463379, 7.223140716552734, 16.04778289794922, 11.032584190368652, 7.14550256729126, 8.545408248901367, 14.565909385681152, 11.247425079345703, 5.851144790649414, 4.206228256225586, 7.170929908752441, 6.772250652313232, 6.437326908111572, 10.415424346923828, 10.226842880249023, 10.951129913330078, 7.4836249351501465, 8.384031295776367, 10.843831062316895, 9.993691444396973, 13.763136863708496, 10.29997730255127, 5.2397050857543945, 5.737247467041016, 9.497917175292969, 12.004732131958008, 7.885673999786377, 6.295645713806152, 9.719785690307617, 3.947657585144043, 7.485284328460693, 10.601339340209961, 9.325949668884277, 7.1477789878845215, 5.738641262054443, 8.391756057739258, 9.35440444946289, 8.632979393005371, 8.785935401916504, 8.569915771484375, 7.40593147277832, 8.21798324584961, 10.887986183166504, 11.224408149719238, 8.870577812194824, 8.543011665344238, 9.94133186340332, 6.165807723999023, 10.621949195861816, 11.89979076385498, 11.245542526245117, 11.284815788269043, 8.054882049560547, 7.454000473022461, 7.390461444854736, 10.763108253479004, 10.498281478881836, 26.02167320251465, 8.987652778625488, 13.902289390563965, 7.446969032287598, 10.481968879699707, 3.947657585144043, 7.3356709480285645, 10.07434368133545, 9.308266639709473, 11.157715797424316, 5.998596668243408, 10.680173873901367, 13.592656135559082, 12.989686012268066, 14.950061798095703, 9.394485473632812, 10.390339851379395, 10.186193466186523, 11.057973861694336, 8.612725257873535, 7.629315376281738, 9.352112770080566, 10.24803638458252, 7.008279800415039, 15.107304573059082, 7.7999043464660645, 9.44384479522705, 11.17492389678955, 7.7629714012146, 9.898716926574707, 22.690383911132812, 11.068676948547363, 15.587939262390137, 8.739066123962402, 7.568291187286377, 8.77381420135498, 12.467148780822754, 12.498154640197754, 12.412808418273926, 11.872035026550293, 8.705089569091797, 11.80417251586914, 14.248519897460938, 10.998889923095703, 11.31847095489502, 9.487860679626465, 8.86269760131836, 9.903995513916016, 10.261829376220703, 15.6592435836792, 12.00109577178955, 11.094369888305664, 11.117142677307129, 14.419486999511719, 10.844197273254395, 12.400370597839355, 12.508257865905762, 9.433293342590332, 14.80894660949707, 8.764939308166504, 9.975831985473633, 6.5163702964782715, 9.57839584350586, 9.44587230682373, 7.207456111907959, 15.318982124328613, 9.384528160095215, 9.189837455749512, 10.300122261047363, 6.640461444854736, 7.170929908752441, 12.372867584228516, 7.380476951599121, 8.378837585449219, 12.082406997680664, 5.319701671600342, 14.05399227142334, 6.945352077484131, 11.156335830688477, 8.39166259765625, 7.708401203155518, 9.578012466430664, 10.09506607055664, 8.429396629333496, 11.667962074279785, 18.04395866394043, 8.140073776245117, 8.76431941986084, 14.646415710449219, 11.410794258117676, 7.8344550132751465, 5.997675895690918, 7.275659561157227, 7.59784460067749, 14.245601654052734, 9.31525993347168, 9.447617530822754, 7.61294412612915, 10.379390716552734, 8.966500282287598, 9.710927963256836, 8.144392013549805, 7.345103740692139, 9.60929012298584, 11.095206260681152, 7.781134605407715, 7.667225360870361, 9.59868335723877, 9.451656341552734, 8.251851081848145, 9.015597343444824, 5.2397050857543945, 14.353148460388184, 12.915051460266113, 10.256600379943848, 9.988838195800781, 11.072405815124512, 12.02350902557373, 10.628360748291016, 14.625107765197754, 6.398837089538574, 10.461554527282715, 11.846923828125, 6.730937957763672, 10.421989440917969, 6.957385063171387, 7.813591003417969, 9.673497200012207, 10.103221893310547, 8.67940616607666, 9.913619041442871, 8.886489868164062, 11.65715217590332, 12.276866912841797, 9.4081449508667, 8.689844131469727, 10.148487091064453, 12.819868087768555, 9.31511402130127, 19.060306549072266, 10.241218566894531, 9.011520385742188, 10.741294860839844].\n"
     ]
    }
   ],
   "source": [
    "runtime_client = session.sagemaker_runtime_client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, ContentType=\"text/libsvm\", Body=payload\n",
    ")\n",
    "result = response[\"Body\"].read().decode(\"ascii\")\n",
    "print(\"Predicted values are {}.\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you’re done with this exercise, please run the cell below to delete the hosted endpoint and avoid any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
